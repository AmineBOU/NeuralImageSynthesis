{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import scipy\n",
    "import h5py\n",
    "import skimage\n",
    "from skimage import io,transform \n",
    "from collections import OrderedDict\n",
    "home = '/home/gatys/'\n",
    "project_dir = home + 'NeuralImageSynthesis/'\n",
    "photo_dir = project_dir + '/Images/Photos/'\n",
    "art_dir = project_dir + '/Images/Paintings/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Python helper functions - should move to python packge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_torch_input(filename, layers, loss_functions, args):\n",
    "    f = h5py.File(filename,'w')\n",
    "    for l,layer in enumerate(layers):\n",
    "        layer_group = f.create_group(layer)\n",
    "        for lf,loss_function in enumerate(loss_functions[l]):\n",
    "            lf_group = layer_group.create_group(loss_function)\n",
    "            for arg in args[l][lf]:\n",
    "                dataset = lf_group.create_dataset(arg, data=args[l][lf][arg])\n",
    "    f.close()\n",
    "    \n",
    "def make_torch_init(filename, init):\n",
    "    f = h5py.File(filename,'w')\n",
    "    f.create_dataset('init', data=init)\n",
    "    f.close()\n",
    "\n",
    "def get_torch_output(filename):\n",
    "    f = h5py.File(filename,'r')\n",
    "    data = f['opt_result']\n",
    "    return data.value\n",
    "    f.close()\n",
    "def get_torch_loss(filename):\n",
    "    f = h5py.File(filename,'r')\n",
    "    data = f['losses']\n",
    "    return data.value\n",
    "    f.close()\n",
    "\n",
    "def list2css(layers):\n",
    "    '''\n",
    "    Takes list of strings and returns comma separated string\n",
    "    '''\n",
    "    css = str()\n",
    "    for l in layers:\n",
    "        css = css+l+','\n",
    "    return css[:-1]\n",
    "\n",
    "def get_activations(images, caffe_model, layers='all', gpu=0):\n",
    "    '''\n",
    "    Function to get neural network activations in response to images from torch.\n",
    "    \n",
    "    :param images: array of images\n",
    "    :param caffe_model: file name of the network .caffemodel file\n",
    "    :param layers: network layers for which the activations should be computed\n",
    "    :return: network activations in response to images\n",
    "    '''\n",
    "    layers = list2css(layers)\n",
    "    tmp_dir = project_dir + 'Tmp/'\n",
    "    images_file_name = tmp_dir + 'images.hdf5'\n",
    "    output_file_name = tmp_dir + 'activations.hdf5'\n",
    "    f = h5py.File(images_file_name, 'w')\n",
    "    f.create_dataset('images', data=images)\n",
    "    f.close()\n",
    "    context = {\n",
    "    'caffe_model': caffe_model,\n",
    "    'images': images_file_name,\n",
    "    'layers': layers,\n",
    "    'gpu': gpu,\n",
    "    'backend': 'cudnn',\n",
    "    'output_file': output_file_name\n",
    "    }\n",
    "\n",
    "    template = ('#!/bin/bash\\n' +\n",
    "                '/usr/local/torch/install/bin/th ComputeActivations.lua ' + \n",
    "                '-caffe_model {caffe_model} ' +\n",
    "                '-images {images} ' + \n",
    "                '-layers {layers} ' + \n",
    "                '-gpu {gpu} ' + \n",
    "                '-backend {backend} ' +\n",
    "                '-output_file {output_file}')\n",
    "\n",
    "    script_name = project_dir + 'get_activations.sh'\n",
    "    with open(script_name, 'w') as script:\n",
    "        script.write(template.format(**context))\n",
    "    #execute script PATH NEEDS TO BE CHANGED ON NEW MACHINE\n",
    "    !cd /home/gatys/NeuralImageSynthesis/ && \\\n",
    "    ./get_activations.sh >/dev/null\n",
    "    \n",
    "    f = h5py.File(output_file_name,'r')\n",
    "    act = OrderedDict()\n",
    "    for key in f.keys():\n",
    "        act[key] = f[key].value.copy()\n",
    "    f.close()\n",
    "    return act\n",
    "\n",
    "def preprocess(image):\n",
    "    assert(image.max() <= 1.)\n",
    "    imagenet_mean = array([0.40760392,  0.45795686,  0.48501961])\n",
    "    image_torch = 255 * (image[:,:,::-1] - imagenet_mean).transpose(2,0,1)\n",
    "    return image_torch\n",
    "\n",
    "def deprocess(image_torch):\n",
    "    imagenet_mean = array([0.40760392,  0.45795686,  0.48501961])\n",
    "    image = (image_torch.transpose(1,2,0)/255. + imagenet_mean)[:,:,::-1]\n",
    "    image[image>1] = 1\n",
    "    image[image<0] = 0\n",
    "    return image\n",
    "\n",
    "def gram_matrix(activations):\n",
    "    n_fm = activations.shape[0]\n",
    "    F = activations.reshape(n_fm,-1)\n",
    "    G = F.dot(F.T) / F[0,:].size\n",
    "    return G\n",
    "\n",
    "import itertools\n",
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "def set_model(name, project_dir):\n",
    "    if name == 'org_pad':\n",
    "        model = project_dir + 'Models/VGG_ILSVRC_19_layers_conv.caffemodel'\n",
    "    elif name == 'org_nopad':\n",
    "        model = project_dir + 'Models/VGG_ILSVRC_19_layers_conv_nopad.caffemodel'\n",
    "    elif name == 'norm_pad':\n",
    "        model = project_dir + 'Models/vgg_normalised.caffemodel'\n",
    "    elif name == 'norm_nopad':\n",
    "        model = project_dir + 'Models/vgg_normalised_nopad.caffemodel'\n",
    "    else:\n",
    "        assert False, 'unknown model name'\n",
    "    return model\n",
    "    \n",
    "def get_patch(image, x, y, h, w):\n",
    "    '''\n",
    "    Returns patch from image\n",
    "    x,y gives upper left corner\n",
    "    h,w gives height and width\n",
    "    '''\n",
    "    patch = image[y:y+h,x:x+w, :].copy()\n",
    "    return patch\n",
    "\n",
    "from skimage import transform\n",
    "import numpy as np\n",
    "def get_rotated_patch(image, x, y, h, w, angle):\n",
    "    '''\n",
    "    returns patch that is rotated by angle (radians) around patch center\n",
    "    '''\n",
    "    tf_rotate = transform.SimilarityTransform(rotation=angle)\n",
    "    tf_shift = transform.SimilarityTransform(translation=[-np.round(x+w/2), -np.round(y+h/2)])\n",
    "    tf_shift_inv = transform.SimilarityTransform(translation=[np.round(x+w/2), np.round(y+h/2)])\n",
    "    image_rotated = transform.warp(image, (tf_shift + (tf_rotate + tf_shift_inv)).inverse)\n",
    "    rotated_patch = get_patch(image_rotated, x, y, h, w)\n",
    "    return rotated_patch\n",
    "\n",
    "def match_color(target_img, source_img, mode='sym'):\n",
    "   '''\n",
    "   Matches the colour distribution of the target image to that of the source image\n",
    "   using a linear transform.\n",
    "   Images are expected to be of form (w,h,c).\n",
    "   Modes are chol, pca or sym for different choices of basis.\n",
    "   '''\n",
    "   mu_t = target_img.mean(0).mean(0)\n",
    "   t = target_img - mu_t\n",
    "   t = t.transpose(2,0,1).reshape(3,-1)\n",
    "   Ct = t.dot(t.T) / t.shape[1]\n",
    "   mu_s = source_img.mean(0).mean(0)\n",
    "   s = source_img - mu_s\n",
    "   s = s.transpose(2,0,1).reshape(3,-1)\n",
    "   Cs = s.dot(s.T) / s.shape[1]\n",
    "   if mode == 'chol':\n",
    "       chol_t = np.linalg.cholesky(Ct)\n",
    "       chol_s = np.linalg.cholesky(Cs)\n",
    "       ts = chol_s.dot(np.linalg.inv(chol_t)).dot(t)\n",
    "   if mode == 'pca':\n",
    "       eva_t, eve_t = np.linalg.eigh(Ct)\n",
    "       Qt = eve_t.dot(np.sqrt(np.diag(eva_t))).dot(eve_t.T)\n",
    "       eva_s, eve_s = np.linalg.eigh(Cs)\n",
    "       Qs = eve_s.dot(np.sqrt(np.diag(eva_s))).dot(eve_s.T)\n",
    "       ts = Qs.dot(np.linalg.inv(Qt)).dot(t)\n",
    "   if mode == 'sym':\n",
    "       eva_t, eve_t = np.linalg.eigh(Ct)\n",
    "       Qt = eve_t.dot(np.sqrt(np.diag(eva_t))).dot(eve_t.T)\n",
    "       Qt_Cs_Qt = Qt.dot(Cs).dot(Qt)\n",
    "       eva_QtCsQt, eve_QtCsQt = np.linalg.eigh(Qt_Cs_Qt)\n",
    "       QtCsQt = eve_QtCsQt.dot(np.sqrt(np.diag(eva_QtCsQt))).dot(eve_QtCsQt.T)\n",
    "       ts = np.linalg.inv(Qt).dot(QtCsQt).dot(np.linalg.inv(Qt)).dot(t)\n",
    "   matched_img = ts.reshape(*target_img.transpose(2,0,1).shape).transpose(1,2,0)\n",
    "   matched_img += mu_s\n",
    "   matched_img[matched_img>1] = 1\n",
    "   matched_img[matched_img<0] = 0\n",
    "   return matched_img\n",
    "\n",
    "def lum_transform(image):\n",
    "    img = image.transpose(2,0,1).reshape(3,-1)\n",
    "    lum = np.array([.299, .587, .114]).dot(img).squeeze()\n",
    "    img = tile(lum[None,:],(3,1)).reshape((3,image.shape[0],image.shape[1]))\n",
    "    return img.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get images and apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get images\n",
    "img_size = 450.\n",
    "conditions = ['content','style']\n",
    "img_names = OrderedDict()\n",
    "img_names['content'] = 'leon_cropped.jpg'\n",
    "img_names['style'] = 'vangogh_starry2.jpg'\n",
    "\n",
    "# img_names['content'] = 'sig_woman.png'\n",
    "# img_names['style'] = 'sig_man_painting_crop.jpg'\n",
    "imgs = OrderedDict()\n",
    "imgs['content'] = imread(photo_dir + img_names['content'])\n",
    "try:\n",
    "    imgs['content'] = transform.pyramid_reduce(imgs['content'], sqrt(float(imgs['content'][:,:,0].size) / img_size**2))\n",
    "except:\n",
    "    imgs['content'] = imgs['content']/255. \n",
    "imgs['style'] = imread(art_dir + img_names['style'])\n",
    "try:\n",
    "    imgs['style'] = transform.pyramid_reduce(imgs['style'], sqrt(float(imgs['style'][:,:,0].size) / img_size**2))\n",
    "except:\n",
    "    imgs['style'] = imgs['style']/255. \n",
    "imgs_torch = OrderedDict()\n",
    "for cond in conditions:\n",
    "    imshow(imgs[cond]);show()\n",
    "    imgs_torch[cond] = preprocess(imgs[cond])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and get the targets for the image synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = 'org_pad'\n",
    "caffe_model = set_model(model_name, project_dir)\n",
    "gpu = 0\n",
    "act = OrderedDict()\n",
    "input_file_name = project_dir + 'Tmp/input.hdf5'\n",
    "init_file_name = project_dir + 'Tmp/init.hdf5'\n",
    "output_file_name = project_dir + 'Tmp/output.hdf'\n",
    "layers = [\n",
    "    'relu1_1',\n",
    "    'relu2_1',\n",
    "    'relu3_1',\n",
    "    'relu4_1',\n",
    "    'relu4_2',\n",
    "    'relu5_1'\n",
    "]\n",
    "loss_functions = [\n",
    "    ['GramMSE'],\n",
    "    ['GramMSE'],\n",
    "    ['GramMSE'],\n",
    "    ['GramMSE'],\n",
    "    ['MSE'],\n",
    "    ['GramMSE']\n",
    "]\n",
    "sw = 1e3\n",
    "cw = 1\n",
    "weights = [\n",
    "    [array([sw/64**2])],\n",
    "    [array([sw/128**2])],\n",
    "    [array([sw/256**2])],\n",
    "    [array([sw/512**2])],\n",
    "    [array([cw])],    \n",
    "    [array([sw/512**2])],\n",
    "]\n",
    "\n",
    "for cond in conditions:\n",
    "    act[cond] = get_activations(imgs_torch[cond],\n",
    "                                caffe_model,\n",
    "                                layers=layers,\n",
    "                                gpu=gpu\n",
    "                               )\n",
    "args = [\n",
    "    [\n",
    "        {'targets': gram_matrix(act['style'][layers[0]])[None,:],\n",
    "          'weights': weights[0][0]}\n",
    "    ],\n",
    "        [\n",
    "        {'targets': gram_matrix(act['style'][layers[1]])[None,:],\n",
    "          'weights': weights[1][0]}\n",
    "    ],\n",
    "        [\n",
    "        {'targets': gram_matrix(act['style'][layers[2]])[None,:],\n",
    "          'weights': weights[2][0]}\n",
    "    ],\n",
    "        [\n",
    "        {'targets': gram_matrix(act['style'][layers[3]])[None,:],\n",
    "          'weights': weights[3][0]}\n",
    "    ],\n",
    "        [\n",
    "        {'targets': act['content'][layers[4]][None,:],\n",
    "          'weights': weights[4][0]}\n",
    "    ],\n",
    "        [\n",
    "        {'targets': gram_matrix(act['style'][layers[5]])[None,:],\n",
    "          'weights': weights[5][0]}\n",
    "        ]\n",
    "]\n",
    "make_torch_input(input_file_name, layers, loss_functions, args)\n",
    "make_torch_init(init_file_name, imgs_torch['content'])\n",
    "# make_torch_init(init_file_name, randn(*imgs_torch['content'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run image synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iter = 500\n",
    "context = {\n",
    "    'caffe_model': caffe_model,\n",
    "    'input_file': input_file_name,\n",
    "    'init_file': init_file_name,\n",
    "    'gpu': gpu,\n",
    "    'max_iter': max_iter,\n",
    "    'backend': 'cudnn',\n",
    "    'print_iter': 50,\n",
    "    'save_iter': 0,\n",
    "    'layer_order': list2css(layers),\n",
    "    'output_file': output_file_name\n",
    "}\n",
    "\n",
    "template = (\n",
    "            '#!/bin/bash\\n' +\n",
    "            'time /usr/local/torch/install/bin/th ImageSynthesis.lua ' + \n",
    "            '-caffe_model {caffe_model} ' +\n",
    "            '-input_file {input_file} ' + \n",
    "            '-init_file {init_file} ' + \n",
    "            '-gpu {gpu} ' + \n",
    "            '-max_iter {max_iter} ' +\n",
    "            '-print_iter {print_iter} ' +\n",
    "            '-save_iter {save_iter} ' +\n",
    "            '-backend {backend} ' + \n",
    "            '-layer_order {layer_order} ' +\n",
    "            '-output_file {output_file}'\n",
    "           )\n",
    "\n",
    "script_name = project_dir + 'run_synthesis.sh'\n",
    "with open(script_name, 'w') as script:\n",
    "    script.write(template.format(**context))\n",
    "#execute script PATH NEEDS TO BE CHANGED ON NEW MACHINE\n",
    "!cd /home/gatys/NeuralImageSynthesis/ && \\\n",
    "./run_synthesis.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load torch output and save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = deprocess(get_torch_output(output_file_name))\n",
    "imshow(output);gcf().set_size_inches(8,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lf = list2css(map(str,loss_functions))\n",
    "lf = str.replace(lf,'[','_')\n",
    "lf = str.replace(lf,']','_')\n",
    "# w = list2css(map(str,weights))\n",
    "# w = str.replace(w,'[','_')\n",
    "# w = str.replace(w,']','_')\n",
    "\n",
    "result_image_name = (\n",
    "'cimg_' + img_names['content'] + \n",
    "'_simg_' + img_names['style'] + \n",
    "'_sz_' + str(img_size) + \n",
    "'_model_' + model_name + \n",
    "'_layers_' + list2css(layers) + \n",
    "'_lf_' + lf +\n",
    "# '_weights_' + w + \n",
    "'.jpg'\n",
    ")\n",
    "imsave(project_dir + 'Results/Images/' + result_image_name, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
